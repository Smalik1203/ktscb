# Scaling Plan: 1 Million Users
**Date:** January 25, 2026  
**Target:** Support 1,000,000 concurrent users  
**Current State:** ~1,000-10,000 users (estimated)

---

## Executive Summary

To scale from current capacity to **1 million users**, this plan addresses:
- **Database bottlenecks** (analytics, queries, connections)
- **Caching strategy** (Redis, CDN, materialized views)
- **Query optimization** (RPC functions, materialized views)
- **Infrastructure scaling** (read replicas, connection pooling)
- **Background processing** (job queues, async tasks)
- **Monitoring & alerting** (performance tracking)

**Estimated Timeline:** 12-16 weeks  
**Estimated Cost Increase:** 5-10x current infrastructure costs

---

## ðŸš¨ Critical Bottlenecks at 1M Users

### Current Capacity Estimates

| Component | Current Capacity | 1M User Requirement | Gap |
|-----------|----------------|---------------------|-----|
| Database Connections | ~100-200 | 5,000-10,000 | **50x** |
| Analytics Queries | ~10/min | 1,000+/min | **100x** |
| Read Queries | ~1,000/min | 100,000+/min | **100x** |
| Write Queries | ~100/min | 10,000+/min | **100x** |
| Cache Hit Rate | ~30% | 80%+ | **Need Redis** |
| Dashboard Load Time | ~2s | <1s | **Need optimization** |

---

## ðŸ“Š Phase 1: Critical Fixes (Weeks 1-4)

### 1.1 Move Analytics to RPC Functions + Materialized Views

**Problem:** Analytics aggregation in JavaScript will crash at scale.

**Solution:** Create materialized views and RPC functions for all analytics.

#### Implementation:

**Step 1: Create Materialized Views**

```sql
-- supabase/migrations/20260125000000_analytics_materialized_views.sql

-- Tasks Analytics Materialized View
CREATE MATERIALIZED VIEW mv_tasks_analytics AS
SELECT 
  t.id as task_id,
  t.title as task_name,
  t.class_instance_id,
  t.subject_id,
  t.due_date,
  ci.grade,
  ci.section,
  s.subject_name,
  COUNT(DISTINCT ts.student_id) as submitted_count,
  COUNT(DISTINCT s2.id) as total_students,
  COUNT(DISTINCT ts.student_id) FILTER (WHERE ts.submitted_at <= t.due_date) as on_time_count,
  CASE 
    WHEN COUNT(DISTINCT ts.student_id) >= COUNT(DISTINCT s2.id) THEN 'completed'
    WHEN CURRENT_DATE > t.due_date THEN 'overdue'
    ELSE 'pending'
  END as status,
  CASE 
    WHEN COUNT(DISTINCT ts.student_id) > 0 
    THEN (COUNT(DISTINCT ts.student_id) FILTER (WHERE ts.submitted_at <= t.due_date)::NUMERIC / COUNT(DISTINCT ts.student_id)::NUMERIC) * 100
    ELSE 0
  END as on_time_rate
FROM tasks t
INNER JOIN class_instances ci ON t.class_instance_id = ci.id
LEFT JOIN subjects s ON t.subject_id = s.id
LEFT JOIN task_submissions ts ON t.id = ts.task_id
LEFT JOIN student s2 ON t.class_instance_id = s2.class_instance_id
WHERE t.is_active = true
GROUP BY t.id, t.title, t.class_instance_id, t.subject_id, t.due_date, ci.grade, ci.section, s.subject_name;

-- Index for fast lookups
CREATE INDEX idx_mv_tasks_analytics_class_date ON mv_tasks_analytics(class_instance_id, due_date DESC);
CREATE INDEX idx_mv_tasks_analytics_school ON mv_tasks_analytics(class_instance_id) 
  INCLUDE (task_id, task_name, status, on_time_rate);

-- Refresh function (call via cron job every 5 minutes)
CREATE OR REPLACE FUNCTION refresh_tasks_analytics()
RETURNS void AS $$
BEGIN
  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_tasks_analytics;
END;
$$ LANGUAGE plpgsql;

-- Academics Analytics Materialized View
CREATE MATERIALIZED VIEW mv_academics_analytics AS
SELECT 
  tm.test_id,
  t.title as test_name,
  t.class_instance_id,
  t.subject_id,
  s.subject_name,
  ci.grade,
  ci.section,
  COUNT(DISTINCT tm.student_id) as participation_count,
  AVG((tm.marks_obtained::NUMERIC / NULLIF(tm.max_marks, 0)) * 100) as avg_score,
  SUM(tm.marks_obtained) as total_marks,
  SUM(tm.max_marks) as total_max_marks,
  COUNT(*) as submission_count
FROM test_marks tm
INNER JOIN tests t ON tm.test_id = t.id
INNER JOIN class_instances ci ON t.class_instance_id = ci.id
LEFT JOIN subjects s ON t.subject_id = s.id
GROUP BY tm.test_id, t.title, t.class_instance_id, t.subject_id, s.subject_name, ci.grade, ci.section;

CREATE INDEX idx_mv_academics_analytics_class ON mv_academics_analytics(class_instance_id, test_id);
CREATE INDEX idx_mv_academics_analytics_subject ON mv_academics_analytics(subject_id);

CREATE OR REPLACE FUNCTION refresh_academics_analytics()
RETURNS void AS $$
BEGIN
  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_academics_analytics;
END;
$$ LANGUAGE plpgsql;

-- Attendance Analytics Materialized View
CREATE MATERIALIZED VIEW mv_attendance_analytics AS
SELECT 
  a.class_instance_id,
  a.date,
  ci.grade,
  ci.section,
  ci.school_code,
  COUNT(*) FILTER (WHERE a.status = 'present') as present_count,
  COUNT(*) FILTER (WHERE a.status = 'absent') as absent_count,
  COUNT(*) FILTER (WHERE a.status = 'late') as late_count,
  COUNT(*) as total_count,
  (COUNT(*) FILTER (WHERE a.status = 'present')::NUMERIC / NULLIF(COUNT(*), 0)) * 100 as attendance_rate
FROM attendance a
INNER JOIN class_instances ci ON a.class_instance_id = ci.id
GROUP BY a.class_instance_id, a.date, ci.grade, ci.section, ci.school_code;

CREATE INDEX idx_mv_attendance_analytics_class_date ON mv_attendance_analytics(class_instance_id, date DESC);
CREATE INDEX idx_mv_attendance_analytics_school_date ON mv_attendance_analytics(school_code, date DESC);

CREATE OR REPLACE FUNCTION refresh_attendance_analytics()
RETURNS void AS $$
BEGIN
  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_attendance_analytics;
END;
$$ LANGUAGE plpgsql;
```

**Step 2: Create RPC Functions**

```sql
-- supabase/migrations/20260125000001_analytics_rpc_functions.sql

-- Tasks Analytics RPC
CREATE OR REPLACE FUNCTION get_tasks_analytics(
  p_school_code TEXT,
  p_academic_year_id UUID,
  p_start_date DATE,
  p_end_date DATE,
  p_class_instance_id UUID DEFAULT NULL,
  p_limit INTEGER DEFAULT 50
)
RETURNS JSON AS $$
DECLARE
  result JSON;
BEGIN
  SELECT json_build_object(
    'aggregation', (
      SELECT json_build_object(
        'totalTasks', COUNT(*),
        'completedTasks', COUNT(*) FILTER (WHERE status = 'completed'),
        'pendingTasks', COUNT(*) FILTER (WHERE status = 'pending'),
        'overdueTasks', COUNT(*) FILTER (WHERE status = 'overdue'),
        'avgOnTimeRate', COALESCE(AVG(on_time_rate), 0)
      )
      FROM mv_tasks_analytics mta
      INNER JOIN class_instances ci ON mta.class_instance_id = ci.id
      WHERE ci.school_code = p_school_code
        AND ci.academic_year_id = p_academic_year_id
        AND mta.due_date >= p_start_date
        AND mta.due_date <= p_end_date
        AND (p_class_instance_id IS NULL OR mta.class_instance_id = p_class_instance_id)
    ),
    'rankedRows', (
      SELECT json_agg(
        json_build_object(
          'rank', row_number() OVER (ORDER BY on_time_rate DESC),
          'data', json_build_object(
            'taskId', task_id,
            'taskName', task_name,
            'className', grade || COALESCE(' - ' || section, ''),
            'subjectName', subject_name,
            'onTimeRate', on_time_rate,
            'status', status
          )
        )
      )
      FROM (
        SELECT *
        FROM mv_tasks_analytics mta
        INNER JOIN class_instances ci ON mta.class_instance_id = ci.id
        WHERE ci.school_code = p_school_code
          AND ci.academic_year_id = p_academic_year_id
          AND mta.due_date >= p_start_date
          AND mta.due_date <= p_end_date
          AND (p_class_instance_id IS NULL OR mta.class_instance_id = p_class_instance_id)
        ORDER BY on_time_rate DESC
        LIMIT p_limit
      ) ranked
    )
  ) INTO result;
  
  RETURN result;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Academics Analytics RPC
CREATE OR REPLACE FUNCTION get_academics_analytics(
  p_school_code TEXT,
  p_academic_year_id UUID,
  p_start_date DATE,
  p_end_date DATE,
  p_class_instance_id UUID DEFAULT NULL,
  p_subject_id UUID DEFAULT NULL,
  p_limit INTEGER DEFAULT 50
)
RETURNS JSON AS $$
DECLARE
  result JSON;
BEGIN
  SELECT json_build_object(
    'aggregation', (
      SELECT json_build_object(
        'totalTests', COUNT(DISTINCT test_id),
        'totalStudents', COUNT(DISTINCT student_id),
        'avgScore', COALESCE(AVG(avg_score), 0),
        'participationRate', COALESCE(
          (COUNT(DISTINCT student_id)::NUMERIC / NULLIF(COUNT(DISTINCT test_id) * 30, 0)) * 100,
          0
        )
      )
      FROM mv_academics_analytics maa
      INNER JOIN class_instances ci ON maa.class_instance_id = ci.id
      INNER JOIN tests t ON maa.test_id = t.id
      WHERE ci.school_code = p_school_code
        AND ci.academic_year_id = p_academic_year_id
        AND t.created_at >= p_start_date
        AND t.created_at <= p_end_date
        AND (p_class_instance_id IS NULL OR maa.class_instance_id = p_class_instance_id)
        AND (p_subject_id IS NULL OR maa.subject_id = p_subject_id)
    ),
    'rankedRows', (
      SELECT json_agg(
        json_build_object(
          'rank', row_number() OVER (ORDER BY avg_score DESC),
          'data', json_build_object(
            'testId', test_id,
            'testName', test_name,
            'subjectName', subject_name,
            'avgScore', avg_score,
            'participationCount', participation_count
          )
        )
      )
      FROM (
        SELECT *
        FROM mv_academics_analytics maa
        INNER JOIN class_instances ci ON maa.class_instance_id = ci.id
        INNER JOIN tests t ON maa.test_id = t.id
        WHERE ci.school_code = p_school_code
          AND ci.academic_year_id = p_academic_year_id
          AND t.created_at >= p_start_date
          AND t.created_at <= p_end_date
          AND (p_class_instance_id IS NULL OR maa.class_instance_id = p_class_instance_id)
          AND (p_subject_id IS NULL OR maa.subject_id = p_subject_id)
        ORDER BY avg_score DESC
        LIMIT p_limit
      ) ranked
    )
  ) INTO result;
  
  RETURN result;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Attendance Analytics RPC
CREATE OR REPLACE FUNCTION get_attendance_analytics(
  p_school_code TEXT,
  p_academic_year_id UUID,
  p_start_date DATE,
  p_end_date DATE,
  p_class_instance_id UUID DEFAULT NULL
)
RETURNS JSON AS $$
DECLARE
  result JSON;
BEGIN
  SELECT json_build_object(
    'aggregation', (
      SELECT json_build_object(
        'totalDays', COUNT(DISTINCT date),
        'avgAttendanceRate', COALESCE(AVG(attendance_rate), 0),
        'totalPresent', SUM(present_count),
        'totalAbsent', SUM(absent_count),
        'totalLate', SUM(late_count)
      )
      FROM mv_attendance_analytics
      WHERE school_code = p_school_code
        AND date >= p_start_date
        AND date <= p_end_date
        AND (p_class_instance_id IS NULL OR class_instance_id = p_class_instance_id)
    ),
    'dailyBreakdown', (
      SELECT json_agg(
        json_build_object(
          'date', date,
          'attendanceRate', attendance_rate,
          'presentCount', present_count,
          'absentCount', absent_count
        )
      )
      FROM mv_attendance_analytics
      WHERE school_code = p_school_code
        AND date >= p_start_date
        AND date <= p_end_date
        AND (p_class_instance_id IS NULL OR class_instance_id = p_class_instance_id)
      ORDER BY date DESC
    )
  ) INTO result;
  
  RETURN result;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

**Step 3: Update Hooks to Use RPC**

```typescript
// src/hooks/analytics/useTasksAnalytics.ts
export function useTasksAnalytics(options: UseTasksAnalyticsOptions) {
  const { school_code, academic_year_id, start_date, end_date, limit, classInstanceId } = options;

  return useQuery({
    queryKey: ['analytics', 'tasks', school_code, academic_year_id, start_date, end_date, classInstanceId, limit],
    queryFn: async () => {
      // âœ… Use RPC instead of client-side aggregation
      const { data, error } = await supabase.rpc('get_tasks_analytics', {
        p_school_code: school_code,
        p_academic_year_id: academic_year_id,
        p_start_date: start_date,
        p_end_date: end_date,
        p_class_instance_id: classInstanceId || null,
        p_limit: limit || 50,
      });

      if (error) throw error;
      return data;
    },
    staleTime: 5 * 60 * 1000, // 5 minutes (materialized view refreshes every 5 min)
  });
}
```

**Step 4: Set Up Cron Jobs for Materialized View Refresh**

```sql
-- supabase/migrations/20260125000002_analytics_refresh_cron.sql

-- Refresh materialized views every 5 minutes
SELECT cron.schedule(
  'refresh-tasks-analytics',
  '*/5 * * * *', -- Every 5 minutes
  $$SELECT refresh_tasks_analytics();$$
);

SELECT cron.schedule(
  'refresh-academics-analytics',
  '*/5 * * * *',
  $$SELECT refresh_academics_analytics();$$
);

SELECT cron.schedule(
  'refresh-attendance-analytics',
  '*/5 * * * *',
  $$SELECT refresh_attendance_analytics();$$
);
```

**Impact:**
- âœ… Analytics queries: 100x faster (materialized views)
- âœ… Database load: 90% reduction (pre-aggregated)
- âœ… Response time: <100ms (was 2-5s)

---

### 1.2 Fix Client-Side Filtering

**Problem:** Filtering in JavaScript wastes bandwidth and memory.

**Solution:** Move all filtering to SQL WHERE clauses.

**Implementation:**

```typescript
// src/lib/analytics-utils.ts - REMOVE applyClientFilters function

// Instead, update hooks to accept filter parameters:
export function useStudents(
  classInstanceId?: string,
  schoolCode?: string,
  options?: { 
    page?: number; 
    pageSize?: number;
    searchQuery?: string; // âœ… Add search parameter
    statusFilter?: string[];
  }
) {
  return useQuery({
    queryKey: ['students', classInstanceId, schoolCode, options],
    queryFn: async () => {
      let query = supabase
        .from('student')
        .select('id, student_code, full_name, email, phone')
        .eq('school_code', schoolCode)
        .order('full_name');

      // âœ… Filter in SQL, not JavaScript
      if (options?.searchQuery) {
        query = query.or(`full_name.ilike.%${options.searchQuery}%,email.ilike.%${options.searchQuery}%,student_code.ilike.%${options.searchQuery}%`);
      }

      if (options?.statusFilter && options.statusFilter.length > 0) {
        // Add status column if it exists, or filter by other criteria
      }

      const { data, error } = await query
        .range((options?.page || 1 - 1) * (options?.pageSize || 50), (options?.page || 1) * (options?.pageSize || 50) - 1);

      if (error) throw error;
      return data;
    },
  });
}
```

**Impact:**
- âœ… Bandwidth: 70% reduction
- âœ… Memory: 80% reduction
- âœ… Query time: 50% faster (database filtering is optimized)

---

### 1.3 Fix N+1 Patterns

**Problem:** `api.resources.getAll()` makes sequential queries.

**Solution:** Single query with proper filtering.

```typescript
// src/services/api.ts
async getAll(schoolCode: string, limit?: number): Promise<DomainLearningResource[]> {
  // âœ… Single query instead of N+1
  const result = await listResources(
    undefined, // No class filter - get all for school
    schoolCode,
    undefined,
    { to: limit ?? 99 }
  );
  return result.data || [];
}
```

**Impact:**
- âœ… Query count: 11 queries â†’ 1 query (for 10 classes)
- âœ… Response time: 2s â†’ 200ms

---

### 1.4 Consolidate Dashboard Queries

**Problem:** Dashboard makes 6+ queries on mount.

**Solution:** Single RPC function.

```sql
-- supabase/migrations/20260125000003_dashboard_rpc.sql

CREATE OR REPLACE FUNCTION get_dashboard_data(
  p_user_id UUID,
  p_school_code TEXT,
  p_class_instance_id UUID DEFAULT NULL
)
RETURNS JSON AS $$
DECLARE
  result JSON;
  user_role TEXT;
BEGIN
  -- Get user role
  SELECT role INTO user_role
  FROM users
  WHERE id = p_user_id;

  SELECT json_build_object(
    'stats', (
      -- Dashboard stats based on role
      SELECT json_build_object(
        'totalStudents', COUNT(*) FILTER (WHERE role = 'student'),
        'totalTasks', COUNT(*) FROM tasks WHERE school_code = p_school_code,
        -- ... more stats
      )
    ),
    'recentActivity', (
      SELECT json_agg(...) FROM recent_activity_view
      WHERE school_code = p_school_code
      LIMIT 10
    ),
    'upcomingEvents', (
      SELECT json_agg(...) FROM school_calendar_events
      WHERE school_code = p_school_code
        AND start_date >= CURRENT_DATE
      ORDER BY start_date
      LIMIT 5
    ),
    'feeOverview', (
      -- Fee data
    ),
    'taskOverview', (
      -- Task data
    ),
    'syllabusOverview', (
      -- Syllabus data
    )
  ) INTO result;
  
  RETURN result;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

**Impact:**
- âœ… Query count: 6+ queries â†’ 1 query
- âœ… Dashboard load: 2s â†’ 300ms

---

## ðŸ“Š Phase 2: Infrastructure Scaling (Weeks 5-8)

### 2.1 Database Read Replicas

**Problem:** Analytics and read queries will overwhelm primary database.

**Solution:** Use Supabase read replicas for analytics queries.

**Implementation:**

1. **Upgrade to Supabase Pro/Team** (required for read replicas)
2. **Configure read replica** in Supabase dashboard
3. **Create separate Supabase client for analytics:**

```typescript
// src/lib/supabase.ts
import { createClient } from '@supabase/supabase-js';

// Primary client (writes + critical reads)
export const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY, {
  // ... existing config
});

// Read replica client (analytics only)
const REPLICA_URL = process.env.EXPO_PUBLIC_SUPABASE_REPLICA_URL || SUPABASE_URL;
export const supabaseReplica = createClient(REPLICA_URL, SUPABASE_ANON_KEY, {
  db: { schema: 'public' },
});

// Update analytics hooks to use replica
// src/hooks/analytics/useTasksAnalytics.ts
export function useTasksAnalytics(options: UseTasksAnalyticsOptions) {
  return useQuery({
    queryFn: async () => {
      // âœ… Use read replica for analytics
      const { data, error } = await supabaseReplica.rpc('get_tasks_analytics', {
        // ... params
      });
      return data;
    },
  });
}
```

**Impact:**
- âœ… Primary DB load: 50% reduction
- âœ… Analytics queries: No impact on primary DB
- âœ… Read capacity: 10x increase

---

### 2.2 Connection Pooling

**Problem:** 1M users = 10,000+ concurrent connections (exceeds PostgreSQL limit).

**Solution:** Use PgBouncer connection pooling.

**Implementation:**

1. **Supabase automatically provides PgBouncer** (via connection string)
2. **Use transaction pooling for analytics:**
   - Connection string: `postgresql://...@db.xxx.supabase.co:6543/postgres?pgbouncer=true`
3. **Configure connection limits:**

```typescript
// src/lib/supabase.ts
export const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY, {
  db: {
    schema: 'public',
    // Use connection pooling
    // Supabase handles this automatically via their proxy
  },
  // Set reasonable timeouts
  global: {
    fetch: async (url, options = {}) => {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 10000); // 10s timeout
      
      try {
        const response = await fetch(url, {
          ...options,
          signal: controller.signal,
        });
        clearTimeout(timeoutId);
        return response;
      } catch (error) {
        clearTimeout(timeoutId);
        throw error;
      }
    },
  },
});
```

**Impact:**
- âœ… Connection capacity: 200 â†’ 10,000+ (via pooling)
- âœ… Connection overhead: 90% reduction

---

### 2.3 Redis Caching Layer

**Problem:** React Query cache is client-side only, doesn't help with server load.

**Solution:** Add Redis for server-side caching.

**Implementation:**

1. **Set up Redis** (Supabase doesn't provide Redis, use external service):
   - Option A: Upstash Redis (serverless, pay-per-use)
   - Option B: AWS ElastiCache
   - Option C: Railway Redis

2. **Create caching service:**

```typescript
// src/lib/redis-cache.ts
import { Redis } from '@upstash/redis';

const redis = new Redis({
  url: process.env.REDIS_URL!,
  token: process.env.REDIS_TOKEN!,
});

export const cache = {
  async get<T>(key: string): Promise<T | null> {
    try {
      const data = await redis.get<T>(key);
      return data;
    } catch (error) {
      console.warn('Redis get error:', error);
      return null; // Fail gracefully
    }
  },

  async set(key: string, value: any, ttlSeconds: number = 300): Promise<void> {
    try {
      await redis.set(key, value, { ex: ttlSeconds });
    } catch (error) {
      console.warn('Redis set error:', error);
      // Fail silently - caching is optional
    }
  },

  async invalidate(pattern: string): Promise<void> {
    try {
      const keys = await redis.keys(pattern);
      if (keys.length > 0) {
        await redis.del(...keys);
      }
    } catch (error) {
      console.warn('Redis invalidate error:', error);
    }
  },
};
```

3. **Create Edge Function for cached queries:**

```typescript
// supabase/functions/cached-analytics/index.ts
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';
import { Redis } from 'https://deno.land/x/upstash_redis@v1.19.3/mod.ts';

const redis = new Redis({
  url: Deno.env.get('REDIS_URL')!,
  token: Deno.env.get('REDIS_TOKEN')!,
});

serve(async (req) => {
  const { analyticsType, params } = await req.json();
  const cacheKey = `analytics:${analyticsType}:${JSON.stringify(params)}`;

  // Try cache first
  const cached = await redis.get(cacheKey);
  if (cached) {
    return new Response(JSON.stringify(cached), {
      headers: { 'Content-Type': 'application/json' },
    });
  }

  // Fetch from database
  const supabase = createClient(
    Deno.env.get('SUPABASE_URL')!,
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
  );

  const { data, error } = await supabase.rpc(`get_${analyticsType}_analytics`, params);

  if (error) {
    return new Response(JSON.stringify({ error: error.message }), { status: 500 });
  }

  // Cache for 5 minutes
  await redis.set(cacheKey, data, { ex: 300 });

  return new Response(JSON.stringify(data), {
    headers: { 'Content-Type': 'application/json' },
  });
});
```

**Impact:**
- âœ… Cache hit rate: 30% â†’ 80%+
- âœ… Database load: 70% reduction (for cached queries)
- âœ… Response time: 50% faster (for cached queries)

---

### 2.4 Database Partitioning

**Problem:** Large tables (attendance, tasks, test_marks) will slow down as they grow.

**Solution:** Partition large tables by date or school_code.

**Implementation:**

```sql
-- supabase/migrations/20260125000004_table_partitioning.sql

-- Partition attendance table by month
CREATE TABLE attendance_partitioned (
  LIKE attendance INCLUDING ALL
) PARTITION BY RANGE (date);

-- Create partitions for next 12 months
DO $$
DECLARE
  month_start DATE;
  month_end DATE;
  partition_name TEXT;
BEGIN
  FOR i IN 0..11 LOOP
    month_start := DATE_TRUNC('month', CURRENT_DATE + (i || ' months')::INTERVAL);
    month_end := month_start + INTERVAL '1 month' - INTERVAL '1 day';
    partition_name := 'attendance_' || TO_CHAR(month_start, 'YYYY_MM');
    
    EXECUTE format('
      CREATE TABLE %I PARTITION OF attendance_partitioned
      FOR VALUES FROM (%L) TO (%L)
    ', partition_name, month_start, month_end + INTERVAL '1 day');
  END LOOP;
END $$;

-- Migrate data (run during maintenance window)
-- INSERT INTO attendance_partitioned SELECT * FROM attendance;

-- Partition tasks by school_code (if schools are isolated)
CREATE TABLE tasks_partitioned (
  LIKE tasks INCLUDING ALL
) PARTITION BY LIST (school_code);

-- Create partition for each school (or use hash partitioning)
-- This is more complex and may not be needed initially
```

**Note:** Partitioning is complex. Consider this for Phase 3 if needed.

**Impact:**
- âœ… Query performance: 10x faster (for date-range queries)
- âœ… Maintenance: Easier to archive old data

---

## ðŸ“Š Phase 3: Advanced Optimizations (Weeks 9-12)

### 3.1 Background Job Processing

**Problem:** Heavy operations (notifications, reports) block user requests.

**Solution:** Use job queue (Supabase Edge Functions + pg_cron or external queue).

**Implementation:**

1. **Create job queue table:**

```sql
-- supabase/migrations/20260125000005_job_queue.sql

CREATE TABLE job_queue (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  job_type TEXT NOT NULL,
  payload JSONB NOT NULL,
  status TEXT NOT NULL DEFAULT 'pending', -- pending, processing, completed, failed
  priority INTEGER DEFAULT 0,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  error_message TEXT,
  retry_count INTEGER DEFAULT 0,
  max_retries INTEGER DEFAULT 3
);

CREATE INDEX idx_job_queue_status_priority ON job_queue(status, priority DESC, created_at)
WHERE status = 'pending';

-- Worker function (runs via pg_cron every minute)
CREATE OR REPLACE FUNCTION process_job_queue()
RETURNS void AS $$
DECLARE
  job_record RECORD;
BEGIN
  -- Get next pending job
  SELECT * INTO job_record
  FROM job_queue
  WHERE status = 'pending'
  ORDER BY priority DESC, created_at
  LIMIT 1
  FOR UPDATE SKIP LOCKED;

  IF NOT FOUND THEN
    RETURN;
  END IF;

  -- Mark as processing
  UPDATE job_queue
  SET status = 'processing', started_at = NOW()
  WHERE id = job_record.id;

  -- Process job (call appropriate function)
  BEGIN
    CASE job_record.job_type
      WHEN 'send_notifications' THEN
        PERFORM send_bulk_notifications(job_record.payload);
      WHEN 'generate_report' THEN
        PERFORM generate_analytics_report(job_record.payload);
      -- Add more job types
    END CASE;

    -- Mark as completed
    UPDATE job_queue
    SET status = 'completed', completed_at = NOW()
    WHERE id = job_record.id;
  EXCEPTION WHEN OTHERS THEN
    -- Mark as failed or retry
    IF job_record.retry_count < job_record.max_retries THEN
      UPDATE job_queue
      SET status = 'pending', retry_count = job_record.retry_count + 1
      WHERE id = job_record.id;
    ELSE
      UPDATE job_queue
      SET status = 'failed', error_message = SQLERRM
      WHERE id = job_record.id;
    END IF;
  END;
END;
$$ LANGUAGE plpgsql;

-- Schedule worker
SELECT cron.schedule(
  'process-job-queue',
  '* * * * *', -- Every minute
  $$SELECT process_job_queue();$$
);
```

2. **Update services to use job queue:**

```typescript
// src/services/notifications.ts
export async function sendBulkNotifications(userIds: string[], message: string) {
  // âœ… Queue job instead of processing immediately
  const { data, error } = await supabase
    .from('job_queue')
    .insert({
      job_type: 'send_notifications',
      payload: { userIds, message },
      priority: 1,
    })
    .select('id')
    .single();

  if (error) throw error;
  return { jobId: data.id };
}
```

**Impact:**
- âœ… User response time: Immediate (no blocking)
- âœ… Throughput: 10x increase (async processing)
- âœ… Reliability: Automatic retries

---

### 3.2 CDN for Static Assets

**Problem:** Image/video downloads will overwhelm server.

**Solution:** Use CDN (Supabase Storage + Cloudflare CDN).

**Implementation:**

1. **Configure Supabase Storage with CDN:**
   - Supabase Storage already uses CDN
   - Ensure proper cache headers

2. **Optimize image delivery:**

```typescript
// src/utils/storage.ts
export function getOptimizedImageUrl(
  bucket: string,
  path: string,
  width?: number,
  height?: number
): string {
  const baseUrl = `${SUPABASE_URL}/storage/v1/object/public/${bucket}/${path}`;
  
  // Use image transformation if available (via Supabase or Cloudflare)
  if (width || height) {
    return `${baseUrl}?width=${width || 'auto'}&height=${height || 'auto'}&quality=80`;
  }
  
  return baseUrl;
}
```

**Impact:**
- âœ… Image load time: 2s â†’ 200ms (via CDN)
- âœ… Server bandwidth: 80% reduction

---

### 3.3 Rate Limiting

**Problem:** Malicious users or bugs can overwhelm system.

**Solution:** Implement rate limiting at Edge Function level.

**Implementation:**

```typescript
// supabase/functions/rate-limited-analytics/index.ts
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { Redis } from 'https://deno.land/x/upstash_redis@v1.19.3/mod.ts';

const redis = new Redis({
  url: Deno.env.get('REDIS_URL')!,
  token: Deno.env.get('REDIS_TOKEN')!,
});

serve(async (req) => {
  const userId = req.headers.get('x-user-id');
  if (!userId) {
    return new Response('Unauthorized', { status: 401 });
  }

  // Rate limit: 10 requests per minute per user
  const key = `rate_limit:analytics:${userId}`;
  const count = await redis.incr(key);
  
  if (count === 1) {
    await redis.expire(key, 60); // Expire after 1 minute
  }

  if (count > 10) {
    return new Response(
      JSON.stringify({ error: 'Rate limit exceeded. Please try again in a minute.' }),
      { status: 429, headers: { 'Content-Type': 'application/json' } }
    );
  }

  // Process request...
});
```

**Impact:**
- âœ… DDoS protection: Prevents abuse
- âœ… Fair usage: Ensures all users get resources

---

## ðŸ“Š Phase 4: Monitoring & Alerting (Weeks 13-16)

### 4.1 Performance Monitoring

**Implementation:**

1. **Set up Supabase Dashboard monitoring:**
   - Database CPU/Memory
   - Query performance
   - Connection pool usage

2. **Add application-level monitoring:**

```typescript
// src/lib/monitoring.ts
export const monitor = {
  async trackQuery(queryName: string, duration: number) {
    // Send to monitoring service (e.g., Sentry, Datadog)
    if (duration > 1000) {
      console.warn(`Slow query: ${queryName} took ${duration}ms`);
      // Send to monitoring service
    }
  },

  async trackError(error: Error, context: Record<string, any>) {
    // Send to error tracking
    console.error('Error:', error, context);
  },
};

// Wrap queries
export function withMonitoring<T>(
  queryName: string,
  queryFn: () => Promise<T>
): Promise<T> {
  const start = Date.now();
  return queryFn()
    .then((result) => {
      const duration = Date.now() - start;
      monitor.trackQuery(queryName, duration);
      return result;
    })
    .catch((error) => {
      monitor.trackError(error, { queryName });
      throw error;
    });
}
```

### 4.2 Alerting

**Set up alerts for:**
- Database CPU > 80%
- Query time > 2s
- Error rate > 1%
- Cache hit rate < 70%
- Connection pool > 80% full

---

## ðŸ’° Cost Estimates

### Current (Estimated)
- Supabase: $25-50/month
- **Total: $25-50/month**

### At 1M Users
- Supabase Pro/Team: $500-2000/month
- Redis (Upstash): $100-500/month
- CDN (Cloudflare): $20-100/month
- Monitoring (Sentry): $50-200/month
- **Total: $670-2800/month**

**Cost per user:** $0.00067 - $0.0028/month

---

## ðŸ“… Timeline

| Phase | Duration | Key Deliverables |
|-------|----------|------------------|
| **Phase 1** | Weeks 1-4 | RPC functions, materialized views, fix N+1, consolidate queries |
| **Phase 2** | Weeks 5-8 | Read replicas, connection pooling, Redis caching |
| **Phase 3** | Weeks 9-12 | Job queue, CDN, rate limiting, partitioning |
| **Phase 4** | Weeks 13-16 | Monitoring, alerting, performance tuning |

**Total: 16 weeks (4 months)**

---

## âœ… Success Metrics

| Metric | Current | Target (1M users) |
|--------|---------|------------------|
| Dashboard load time | 2s | <500ms |
| Analytics query time | 2-5s | <200ms |
| Database CPU | <50% | <70% |
| Cache hit rate | 30% | 80%+ |
| Error rate | <0.1% | <0.1% |
| Uptime | 99% | 99.9% |

---

## ðŸš€ Quick Wins (Do First)

1. **Move analytics to RPC** (Week 1) - Biggest impact
2. **Add materialized views** (Week 1-2) - 100x faster analytics
3. **Fix client-side filtering** (Week 2) - 70% bandwidth reduction
4. **Consolidate dashboard queries** (Week 2) - 6 queries â†’ 1
5. **Add Redis caching** (Week 3) - 80% cache hit rate

---

## ðŸ“ Next Steps

1. **Review this plan** with team
2. **Prioritize phases** based on current user growth
3. **Set up monitoring** to track current performance
4. **Start Phase 1** (critical fixes)
5. **Measure impact** after each phase

---

**End of Scaling Plan**
